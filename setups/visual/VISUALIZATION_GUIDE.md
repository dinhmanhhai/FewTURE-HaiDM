# HÆ°á»›ng dáº«n Visualize Attention cá»§a Model

CÃ³ nhiá»u cÃ¡ch Ä‘á»ƒ kiá»ƒm tra xem model Ä‘ang táº­p trung vÃ o Ä‘Ã¢u trong hÃ¬nh áº£nh:

## ğŸ“‹ CÃ¡c phÆ°Æ¡ng phÃ¡p visualization

### 1. **Grad-CAM (Gradient-weighted Class Activation Mapping)**
- **CÃ¡ch hoáº¡t Ä‘á»™ng**: Sá»­ dá»¥ng gradients Ä‘á»ƒ xÃ¡c Ä‘á»‹nh vÃ¹ng quan trá»ng
- **Æ¯u Ä‘iá»ƒm**: Hoáº¡t Ä‘á»™ng vá»›i má»i model, khÃ´ng cáº§n thay Ä‘á»•i architecture
- **File**: `visualize_simple.py`

### 2. **Patch Attention Maps**
- **CÃ¡ch hoáº¡t Ä‘á»™ng**: Extract attention weights tá»« CLS token Ä‘áº¿n cÃ¡c patches
- **Æ¯u Ä‘iá»ƒm**: Trá»±c tiáº¿p tá»« attention mechanism cá»§a Transformer
- **File**: `visualize_simple.py`

### 3. **Window-based MSA Attention**
- **CÃ¡ch hoáº¡t Ä‘á»™ng**: Attention weights tá»« Window-based Multi-Head Self-Attention
- **Æ¯u Ä‘iá»ƒm**: Chi tiáº¿t hÆ¡n, hiá»ƒn thá»‹ attention trong tá»«ng window
- **File**: `visualize_attention.py`

### 4. **Channel Attention Maps**
- **CÃ¡ch hoáº¡t Ä‘á»™ng**: Attention weights tá»« Channel Attention Block (CAB)
- **Æ¯u Ä‘iá»ƒm**: Hiá»ƒn thá»‹ kÃªnh nÃ o Ä‘Æ°á»£c model chÃº Ã½
- **File**: `visualize_attention.py`

### 5. **Reconstruction Attention**
- **CÃ¡ch hoáº¡t Ä‘á»™ng**: Attention weights tá»« Dual Reconstruction module
- **Æ¯u Ä‘iá»ƒm**: Hiá»ƒn thá»‹ cÃ¡ch model reconstruct features
- **File**: `visualize_attention.py`

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Script Ä‘Æ¡n giáº£n (khuyáº¿n nghá»‹)

```bash
python visualize_simple.py \
    --image_path /path/to/your/image.jpg \
    --model_path /path/to/checkpoint.pth \
    --arch vit_small \
    --patch_size 16 \
    --image_size 224 \
    --output_dir ./attention_viz
```

**Káº¿t quáº£:**
- `gradcam_overlay.png`: Grad-CAM visualization
- `patch_attention_overlay.png`: Patch attention tá»« CLS token
- `patch_grid.png`: Grid visualization cá»§a patches

### Script Ä‘áº§y Ä‘á»§ (cho multi-attention architecture)

```bash
python visualize_attention.py \
    --image_path /path/to/your/image.jpg \
    --model_path /path/to/checkpoint.pth \
    --arch vit_small \
    --patch_size 16 \
    --image_size 224 \
    --use_mab True \
    --use_ocab True \
    --use_drff True \
    --output_dir ./attention_viz
```

## ğŸ“Š Giáº£i thÃ­ch káº¿t quáº£

### MÃ u sáº¯c trong heatmap:
- **ğŸ”´ Äá»**: VÃ¹ng cÃ³ attention cao (model táº­p trung nhiá»u)
- **ğŸŸ¡ VÃ ng**: VÃ¹ng cÃ³ attention trung bÃ¬nh
- **ğŸ”µ Xanh**: VÃ¹ng cÃ³ attention tháº¥p (model Ã­t chÃº Ã½)

### CÃ¡c loáº¡i visualization:

1. **Original Image**: HÃ¬nh áº£nh gá»‘c
2. **Attention Heatmap**: Báº£n Ä‘á»“ nhiá»‡t cá»§a attention
3. **Overlay**: Káº¿t há»£p hÃ¬nh áº£nh vÃ  heatmap Ä‘á»ƒ dá»… nhÃ¬n

## ğŸ” So sÃ¡nh vá»›i máº¯t ngÆ°á»i

Äá»ƒ so sÃ¡nh vá»›i cÃ¡ch báº¡n nhÃ¬n:

1. **Xem overlay visualization**: MÃ u Ä‘á» = vÃ¹ng model chÃº Ã½
2. **So sÃ¡nh vá»›i vÃ¹ng báº¡n nhÃ¬n**: 
   - Náº¿u trÃ¹ng khá»›p â†’ Model Ä‘ang há»c Ä‘Ãºng
   - Náº¿u khÃ¡c â†’ CÃ³ thá»ƒ model Ä‘ang há»c features khÃ¡c

3. **Kiá»ƒm tra patch grid**: Xem patches nÃ o Ä‘Æ°á»£c highlight

## ğŸ’¡ Tips

1. **Thá»­ nhiá»u hÃ¬nh áº£nh**: Model cÃ³ thá»ƒ táº­p trung khÃ¡c nhau vá»›i cÃ¡c loáº¡i áº£nh khÃ¡c nhau
2. **So sÃ¡nh cÃ¡c layers**: Attention á»Ÿ layers khÃ¡c nhau cÃ³ thá»ƒ khÃ¡c nhau
3. **Kiá»ƒm tra vá»›i support/query**: Trong few-shot learning, so sÃ¡nh attention giá»¯a support vÃ  query

## ğŸ› ï¸ Troubleshooting

### Lá»—i: "Cannot find model"
- Kiá»ƒm tra `--arch` cÃ³ Ä‘Ãºng khÃ´ng
- Äáº£m báº£o model Ä‘Æ°á»£c import Ä‘Ãºng trong `models/__init__.py`

### Lá»—i: "Checkpoint not found"
- Kiá»ƒm tra Ä‘Æ°á»ng dáº«n checkpoint
- Äáº£m báº£o checkpoint cÃ³ keys: `params`, `state_dict`, hoáº·c trá»±c tiáº¿p lÃ  state_dict

### Attention map toÃ n mÃ u xanh/Ä‘á»
- CÃ³ thá»ƒ do normalization, script sáº½ tá»± Ä‘á»™ng normalize
- Thá»­ Ä‘iá»u chá»‰nh `alpha` trong overlay

## ğŸ“ VÃ­ dá»¥ output

```
[1/4] Loading image: test_image.jpg
   Image size: (224, 224)

[2/4] Loading model: vit_small
   Loading checkpoint: checkpoint.pth
   âœ“ Model loaded successfully

[3/4] Computing Grad-CAM...
âœ“ Saved: ./attention_viz/gradcam_overlay.png
   Extracting patch attention...
âœ“ Saved: ./attention_viz/patch_attention_overlay.png
âœ“ Saved: ./attention_viz/patch_grid.png

[4/4] Visualization complete!
```

## ğŸ¯ Next Steps

1. Cháº¡y visualization trÃªn nhiá»u hÃ¬nh áº£nh
2. So sÃ¡nh attention giá»¯a cÃ¡c models khÃ¡c nhau
3. PhÃ¢n tÃ­ch xem model cÃ³ Ä‘ang há»c Ä‘Ãºng features khÃ´ng
4. Äiá»u chá»‰nh training náº¿u cáº§n

